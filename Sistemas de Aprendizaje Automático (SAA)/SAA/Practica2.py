import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.pipeline import make_pipeline

# 1. Carga y ExploraciÃ³n de Datos
# Descargamos el dataset de viviendas de California
dataset = fetch_california_housing()
df = pd.DataFrame(dataset.data, columns=dataset.feature_names)
df['PRICE'] = dataset.target

print("\nğŸ“Š Primeras filas del dataset:")
print(df.head())

# 2. Preprocesamiento de Datos
# Seleccionamos caracterÃ­sticas relevantes
X = df[['MedInc', 'AveRooms', 'HouseAge', 'AveOccup']]
y = df['PRICE']

# NormalizaciÃ³n de los datos para mejorar el rendimiento del modelo
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Dividimos los datos en entrenamiento (80%) y prueba (20%)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# 3. ConstrucciÃ³n del Modelo con caracterÃ­sticas polinÃ³micas
print("\nğŸ” Entrenando modelo de regresiÃ³n polinÃ³mica...")
poly = PolynomialFeatures(degree=2, include_bias=False)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

# Creamos y entrenamos el modelo de regresiÃ³n lineal
model = LinearRegression()
model.fit(X_train_poly, y_train)
y_pred = model.predict(X_test_poly)

# 4. EvaluaciÃ³n del Modelo
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("\nğŸ“ˆ Resultados de la regresiÃ³n polinÃ³mica:")
print(f'â¡ï¸ Error CuadrÃ¡tico Medio (MSE): {mse:.2f}')
print(f'â¡ï¸ Coeficiente de DeterminaciÃ³n (RÂ²): {r2:.2f}')

# VisualizaciÃ³n de resultados
plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel("Valores reales")
plt.ylabel("Predicciones")
plt.title("RegresiÃ³n Lineal PolinÃ³mica: Predicciones vs Valores Reales")
plt.show()

# 5. AnÃ¡lisis y Mejoras
print("\nâš–ï¸ Comparando con otros modelos...")

# ComparaciÃ³n con RegresiÃ³n Ridge
ridge = Ridge(alpha=1.0)
ridge.fit(X_train_poly, y_train)
y_pred_ridge = ridge.predict(X_test_poly)

# ComparaciÃ³n con RegresiÃ³n Lasso
lasso = Lasso(alpha=0.1)
lasso.fit(X_train_poly, y_train)
y_pred_lasso = lasso.predict(X_test_poly)

# MÃ©tricas de Ridge y Lasso
mse_ridge = mean_squared_error(y_test, y_pred_ridge)
r2_ridge = r2_score(y_test, y_pred_ridge)

mse_lasso = mean_squared_error(y_test, y_pred_lasso)
r2_lasso = r2_score(y_test, y_pred_lasso)

print("\nğŸ“Š ComparaciÃ³n de Modelos:")
print(f'ğŸ”¹ Ridge - MSE: {mse_ridge:.2f}, RÂ²: {r2_ridge:.2f}')
print(f'ğŸ”¹ Lasso - MSE: {mse_lasso:.2f}, RÂ²: {r2_lasso:.2f}')

print("\nâœ… AnÃ¡lisis final: Ridge parece ser una mejor opciÃ³n que Lasso en este caso.")
